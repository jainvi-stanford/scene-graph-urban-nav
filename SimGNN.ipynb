{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y08trPfN2lMl"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAvBFl-fyWr0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "torch_version = str(torch.__version__)\n",
        "scatter_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "sparse_src = f\"https://pytorch-geometric.com/whl/torch-{torch_version}.html\"\n",
        "!pip install torch-scatter -f $scatter_src\n",
        "!pip install torch-sparse -f $sparse_src\n",
        "!pip install torch-geometric\n",
        "!pip install texttable\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7EHWHZJ2ouo"
      },
      "source": [
        "# Custom Dataset Class\n",
        "\n",
        "This is the custom dataset class that we use to load our own scene graphs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLAKNgqE37zx"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"\n",
        "    Custom Pytorch Geometric dataset class for loading scene graphs\n",
        "    \"\"\"\n",
        "    def __init__(self, df, output_folder_path):\n",
        "        \"\"\"\n",
        "        Initialize the dataset\n",
        "        :param df: Dataframe containing graph dataset information (filepaths and coordinates).\n",
        "        :param output_folder_path: Path to the folder containing scene graph JSON files.\n",
        "        \"\"\"\n",
        "        super(CustomDataset, self).__init__()\n",
        "        self.output_folder_path = output_folder_path\n",
        "        self.df = df\n",
        "        # Calculate the normalized distance matrix\n",
        "        self.ndist_matrix = self.calculate_ndist_matrix()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Get the length of the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Get an item from the dataset.\n",
        "        :param idx: Index of the item.\n",
        "        :return data: PyTorch Geometric Data object containing graph information.\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx): # Handle tensor indices\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        if isinstance(idx, slice):  # Handle slice indexing\n",
        "            idx = list(range(*idx.indices(len(self))))\n",
        "\n",
        "        if isinstance(idx, list):  # Handle list of indices\n",
        "            return [self[i] for i in idx]\n",
        "\n",
        "        # Load graph data from JSON file\n",
        "        graph_filename = os.path.join(self.output_folder_path,\n",
        "                                      self.df.iloc[idx]['graph_filename'])\n",
        "        with open(graph_filename, 'r') as f:\n",
        "            graph_data = json.load(f)\n",
        "\n",
        "        # Extract node features and create node features tensor\n",
        "        node_features = []\n",
        "        for node in graph_data['nodes']:\n",
        "            node_features.append(node['embedding'])\n",
        "        x = torch.tensor(node_features, dtype=torch.float32)\n",
        "\n",
        "        # Extract edge information and create edge index and edge features tensor\n",
        "        edge_index = []\n",
        "        edge_attr = []\n",
        "        # Create id to index mapping\n",
        "        node_id_map = {node['id']: i for i, node in enumerate(graph_data['nodes'])}\n",
        "\n",
        "        for edge in graph_data['edges']:\n",
        "            source_index = node_id_map.get(edge['source'])\n",
        "            target_index = node_id_map.get(edge['target'])\n",
        "            if source_index is not None and target_index is not None:\n",
        "                edge_index.append([source_index, target_index])\n",
        "                edge_attr.append(edge['embedding'])\n",
        "\n",
        "        edge_index = torch.tensor(edge_index, dtype=torch.long).t().contiguous()\n",
        "        edge_attr = torch.tensor(edge_attr, dtype=torch.float32)\n",
        "\n",
        "        # Create PyTorch Geometric Data object\n",
        "        # we add the idx as an attribute to help with indexing for the ndist_matrix\n",
        "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, i=idx)\n",
        "\n",
        "        # Add other columns as attributes\n",
        "        for col in self.df.columns:\n",
        "            setattr(data, col, self.df.iloc[idx][col])\n",
        "\n",
        "        return data\n",
        "\n",
        "    def calculate_ndist_matrix(self):\n",
        "        \"\"\"\n",
        "        Calculate the normalized distance matrix based on L2 distance between XYZ coordinates.\n",
        "        \"\"\"\n",
        "        xyz = self.df[['X', 'Y', 'Z']].values\n",
        "        # Calculate L2 distances\n",
        "        distance_matrix = np.linalg.norm(xyz[:, None, :]\n",
        "                                         - xyz[None, :, :], axis=-1)\n",
        "        max_dist = np.max(distance_matrix)\n",
        "        normalized_matrix = distance_matrix / max_dist\n",
        "        return torch.tensor(normalized_matrix, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SimGNN"
      ],
      "metadata": {
        "id": "ZtGlT1L3uwVa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zZdoNrg2sEt"
      },
      "source": [
        "## Extended SimGNN Modules\n",
        "These modules are directly borrowed from https://github.com/gospodima/Extended-SimGNN/tree/master, and cover the Attention layer, the Neural Tensor Network and the Diffpool modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIw5MMIOzYJR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from math import ceil\n",
        "from torch.nn import Linear, ReLU\n",
        "from torch_geometric.nn import (\n",
        "    DenseSAGEConv,\n",
        "    DenseGCNConv,\n",
        "    DenseGINConv,\n",
        "    dense_diff_pool,\n",
        "    JumpingKnowledge,\n",
        ")\n",
        "from torch_scatter import scatter_mean, scatter_add\n",
        "\n",
        "\n",
        "class AttentionModule(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    SimGNN Attention Module to make a pass on graph.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        :param args: Arguments object.\n",
        "        \"\"\"\n",
        "        super(AttentionModule, self).__init__()\n",
        "        self.args = args\n",
        "        self.setup_weights()\n",
        "        self.init_parameters()\n",
        "\n",
        "    def setup_weights(self):\n",
        "        \"\"\"\n",
        "        Defining weights.\n",
        "        \"\"\"\n",
        "        self.weight_matrix = torch.nn.Parameter(\n",
        "            torch.Tensor(self.args.filters_3, self.args.filters_3)\n",
        "        )\n",
        "\n",
        "    def init_parameters(self):\n",
        "        \"\"\"\n",
        "        Initializing weights.\n",
        "        \"\"\"\n",
        "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
        "\n",
        "    def forward(self, x, batch, size=None):\n",
        "        \"\"\"\n",
        "        Making a forward propagation pass to create a graph level representation.\n",
        "        :param x: Result of the GNN.\n",
        "        :param size: Dimension size for scatter_\n",
        "        :param batch: Batch vector, which assigns each node to a specific example\n",
        "        :return representation: A graph level representation matrix.\n",
        "        \"\"\"\n",
        "        size = batch[-1].item() + 1 if size is None else size\n",
        "        mean = scatter_mean(x, batch, dim=0, dim_size=size)\n",
        "        transformed_global = torch.tanh(torch.mm(mean, self.weight_matrix))\n",
        "\n",
        "        coefs = torch.sigmoid((x * transformed_global[batch]).sum(dim=1))\n",
        "        weighted = coefs.unsqueeze(-1) * x\n",
        "\n",
        "        return scatter_add(weighted, batch, dim=0, dim_size=size)\n",
        "\n",
        "    def get_coefs(self, x):\n",
        "        mean = x.mean(dim=0)\n",
        "        transformed_global = torch.tanh(torch.matmul(mean, self.weight_matrix))\n",
        "\n",
        "        return torch.sigmoid(torch.matmul(x, transformed_global))\n",
        "\n",
        "\n",
        "class DenseAttentionModule(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    SimGNN Dense Attention Module to make a pass on graph.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        :param args: Arguments object.\n",
        "        \"\"\"\n",
        "        super(DenseAttentionModule, self).__init__()\n",
        "        self.args = args\n",
        "        self.setup_weights()\n",
        "        self.init_parameters()\n",
        "\n",
        "    def setup_weights(self):\n",
        "        \"\"\"\n",
        "        Defining weights.\n",
        "        \"\"\"\n",
        "        self.weight_matrix = torch.nn.Parameter(\n",
        "            torch.Tensor(self.args.filters_3, self.args.filters_3)\n",
        "        )\n",
        "\n",
        "    def init_parameters(self):\n",
        "        \"\"\"\n",
        "        Initializing weights.\n",
        "        \"\"\"\n",
        "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        \"\"\"\n",
        "        Making a forward propagation pass to create a graph level representation.\n",
        "        :param x: Result of the GNN.\n",
        "        :param mask: Mask matrix indicating the valid nodes for each graph.\n",
        "        :return representation: A graph level representation matrix.\n",
        "        \"\"\"\n",
        "        B, N, _ = x.size()\n",
        "\n",
        "        if mask is not None:\n",
        "            num_nodes = mask.view(B, N).sum(dim=1).unsqueeze(-1)\n",
        "            mean = x.sum(dim=1) / num_nodes.to(x.dtype)\n",
        "        else:\n",
        "            mean = x.mean(dim=1)\n",
        "\n",
        "        transformed_global = torch.tanh(torch.mm(mean, self.weight_matrix))\n",
        "\n",
        "        koefs = torch.sigmoid(torch.matmul(x, transformed_global.unsqueeze(-1)))\n",
        "        weighted = koefs * x\n",
        "\n",
        "        if mask is not None:\n",
        "            weighted = weighted * mask.view(B, N, 1).to(x.dtype)\n",
        "\n",
        "        return weighted.sum(dim=1)\n",
        "\n",
        "\n",
        "class TensorNetworkModule(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    SimGNN Tensor Network module to calculate similarity vector.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        :param args: Arguments object.\n",
        "        \"\"\"\n",
        "        super(TensorNetworkModule, self).__init__()\n",
        "        self.args = args\n",
        "        self.setup_weights()\n",
        "        self.init_parameters()\n",
        "\n",
        "    def setup_weights(self):\n",
        "        \"\"\"\n",
        "        Defining weights.\n",
        "        \"\"\"\n",
        "        self.weight_matrix = torch.nn.Parameter(\n",
        "            torch.Tensor(\n",
        "                self.args.filters_3, self.args.filters_3, self.args.tensor_neurons\n",
        "            )\n",
        "        )\n",
        "        self.weight_matrix_block = torch.nn.Parameter(\n",
        "            torch.Tensor(self.args.tensor_neurons, 2 * self.args.filters_3)\n",
        "        )\n",
        "        self.bias = torch.nn.Parameter(torch.Tensor(self.args.tensor_neurons, 1))\n",
        "\n",
        "    def init_parameters(self):\n",
        "        \"\"\"\n",
        "        Initializing weights.\n",
        "        \"\"\"\n",
        "        torch.nn.init.xavier_uniform_(self.weight_matrix)\n",
        "        torch.nn.init.xavier_uniform_(self.weight_matrix_block)\n",
        "        torch.nn.init.xavier_uniform_(self.bias)\n",
        "\n",
        "    def forward(self, embedding_1, embedding_2):\n",
        "        \"\"\"\n",
        "        Making a forward propagation pass to create a similarity vector.\n",
        "        :param embedding_1: Result of the 1st embedding after attention.\n",
        "        :param embedding_2: Result of the 2nd embedding after attention.\n",
        "        :return scores: A similarity score vector.\n",
        "        \"\"\"\n",
        "        batch_size = len(embedding_1)\n",
        "        scoring = torch.matmul(\n",
        "            embedding_1, self.weight_matrix.view(self.args.filters_3, -1)\n",
        "        )\n",
        "        scoring = scoring.view(batch_size, self.args.filters_3, -1).permute([0, 2, 1])\n",
        "        scoring = torch.matmul(\n",
        "            scoring, embedding_2.view(batch_size, self.args.filters_3, 1)\n",
        "        ).view(batch_size, -1)\n",
        "        combined_representation = torch.cat((embedding_1, embedding_2), 1)\n",
        "        block_scoring = torch.t(\n",
        "            torch.mm(self.weight_matrix_block, torch.t(combined_representation))\n",
        "        )\n",
        "        scores = F.relu(scoring + block_scoring + self.bias.view(-1))\n",
        "        return scores\n",
        "\n",
        "\n",
        "class Block(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, mode=\"cat\"):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        # self.conv1 = DenseSAGEConv(in_channels, hidden_channels)\n",
        "        # self.conv2 = DenseSAGEConv(hidden_channels, out_channels)\n",
        "\n",
        "        # self.conv1 = DenseGCNConv(in_channels, hidden_channels)\n",
        "        # self.conv2 = DenseGCNConv(hidden_channels, out_channels)\n",
        "\n",
        "        nn1 = torch.nn.Sequential(\n",
        "            Linear(in_channels, hidden_channels),\n",
        "            ReLU(),\n",
        "            Linear(hidden_channels, hidden_channels),\n",
        "        )\n",
        "\n",
        "        nn2 = torch.nn.Sequential(\n",
        "            Linear(hidden_channels, out_channels),\n",
        "            ReLU(),\n",
        "            Linear(out_channels, out_channels),\n",
        "        )\n",
        "\n",
        "        self.conv1 = DenseGINConv(nn1, train_eps=True)\n",
        "        self.conv2 = DenseGINConv(nn2, train_eps=True)\n",
        "\n",
        "        self.jump = JumpingKnowledge(mode)\n",
        "        if mode == \"cat\":\n",
        "            self.lin = Linear(hidden_channels + out_channels, out_channels)\n",
        "        else:\n",
        "            self.lin = Linear(out_channels, out_channels)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.conv1.reset_parameters()\n",
        "        self.conv2.reset_parameters()\n",
        "        self.lin.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj, mask=None, add_loop=True):\n",
        "        x1 = F.relu(self.conv1(x, adj, mask, add_loop))\n",
        "        x2 = F.relu(self.conv2(x1, adj, mask, add_loop))\n",
        "        return self.lin(self.jump([x1, x2]))\n",
        "\n",
        "\n",
        "class DiffPool(torch.nn.Module):\n",
        "    def __init__(self, args, num_nodes=10, num_layers=4, hidden=16, ratio=0.25):\n",
        "        super(DiffPool, self).__init__()\n",
        "\n",
        "        self.args = args\n",
        "        num_features = self.args.filters_3\n",
        "\n",
        "        self.att = DenseAttentionModule(self.args)\n",
        "\n",
        "        num_nodes = ceil(ratio * num_nodes)\n",
        "        self.embed_block1 = Block(num_features, hidden, hidden)\n",
        "        self.pool_block1 = Block(num_features, hidden, num_nodes)\n",
        "\n",
        "        self.embed_blocks = torch.nn.ModuleList()\n",
        "        self.pool_blocks = torch.nn.ModuleList()\n",
        "        for i in range((num_layers // 2) - 1):\n",
        "            num_nodes = ceil(ratio * num_nodes)\n",
        "            self.embed_blocks.append(Block(hidden, hidden, hidden))\n",
        "            self.pool_blocks.append(Block(hidden, hidden, num_nodes))\n",
        "        self.jump = JumpingKnowledge(mode=\"cat\")\n",
        "        self.lin1 = Linear((len(self.embed_blocks) + 1) * hidden, hidden)\n",
        "        self.lin2 = Linear(hidden, num_features)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.embed_block1.reset_parameters()\n",
        "        self.pool_block1.reset_parameters()\n",
        "        for block1, block2 in zip(self.embed_blocks, self.pool_blocks):\n",
        "            block1.reset_parameters()\n",
        "            block2.reset_parameters()\n",
        "        self.jump.reset_parameters()\n",
        "        self.lin1.reset_parameters()\n",
        "        self.lin2.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj, mask):\n",
        "        s = self.pool_block1(x, adj, mask, add_loop=True)\n",
        "        x = F.relu(self.embed_block1(x, adj, mask, add_loop=True))\n",
        "\n",
        "        xs = [self.att(x, mask)]\n",
        "        x, adj, _, _ = dense_diff_pool(x, adj, s, mask)\n",
        "\n",
        "        for i, (embed, pool) in enumerate(zip(self.embed_blocks, self.pool_blocks)):\n",
        "            s = pool(x, adj)\n",
        "            x = F.relu(embed(x, adj))\n",
        "            xs.append(self.att(x))\n",
        "            if i < (len(self.embed_blocks) - 1):\n",
        "                x, adj, _, _ = dense_diff_pool(x, adj, s)\n",
        "\n",
        "        x = self.jump(xs)\n",
        "        x = F.relu(self.lin1(x))\n",
        "        x = self.lin2(x)\n",
        "        return x\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils\n",
        "These cover functions for evaluation metrics and helper function for batch creation."
      ],
      "metadata": {
        "id": "nmSwIUU0u8NY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68rPAwjrzZaZ"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "import torch\n",
        "import random\n",
        "from texttable import Texttable\n",
        "from torch_geometric.utils import erdos_renyi_graph, to_undirected, to_networkx\n",
        "from torch_geometric.data import Data\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def calculate_ranking_correlation(rank_corr_function, prediction, target):\n",
        "    \"\"\"\n",
        "    Calculating specific ranking correlation for predicted values.\n",
        "    :param rank_corr_function: Ranking correlation function.\n",
        "    :param prediction: Vector of predicted values.\n",
        "    :param target: Vector of ground-truth values.\n",
        "    :return ranking: Ranking correlation value.\n",
        "    \"\"\"\n",
        "    temp = prediction.argsort()\n",
        "    r_prediction = np.empty_like(temp)\n",
        "    r_prediction[temp] = np.arange(len(prediction))\n",
        "\n",
        "    temp = target.argsort()\n",
        "    r_target = np.empty_like(temp)\n",
        "    r_target[temp] = np.arange(len(target))\n",
        "\n",
        "    return rank_corr_function(r_prediction, r_target).correlation\n",
        "\n",
        "def calculate_prec_at_k(k, prediction, target):\n",
        "    \"\"\"\n",
        "    Calculating precision at k using distance metrics.\n",
        "    :param k: Number of top items to consider.\n",
        "    :param prediction: Vector of predicted distance values.\n",
        "    :param target: Vector of ground-truth distance values.\n",
        "    :return: Precision at k.\n",
        "    \"\"\"\n",
        "    # Adjust k in case of ties at the k-th distance value\n",
        "    target_increase = np.sort(target)  # Sort in ascending order (lower distance = more relevant)\n",
        "    target_value_sel = (target_increase <= target_increase[k - 1]).sum()\n",
        "    target_k = max(k, target_value_sel)\n",
        "\n",
        "    # Select top-k indices based on smallest distances\n",
        "    best_k_pred = prediction.argsort()[:k]\n",
        "    best_k_target = target.argsort()[:target_k]\n",
        "\n",
        "    return len(set(best_k_pred).intersection(set(best_k_target))) / k\n",
        "\n",
        "def random_sample_from_closest_vectorized(given_numbers, number_list, neighbor_sample_size=5, close_fraction=0.5):\n",
        "    # Convert to NumPy arrays\n",
        "    batch_size = len(given_numbers)\n",
        "    given_numbers = np.array(given_numbers)[:int(close_fraction*batch_size)]\n",
        "    number_list = np.array(number_list)\n",
        "\n",
        "    # Step 1: Compute absolute differences for all pairs\n",
        "    differences = np.abs(given_numbers[:, None] - number_list[None, :])\n",
        "\n",
        "    # Step 2: Get the indices of the k smallest differences for each given number\n",
        "    closest_indices = np.argsort(differences, axis=1)[:, :neighbor_sample_size]\n",
        "\n",
        "    # Step 3: Select the closest numbers\n",
        "    closest_numbers = number_list[closest_indices]\n",
        "\n",
        "    # Step 4: Randomly sample one number for each row\n",
        "    random_choices = [np.random.choice(row) for row in closest_numbers] + [np.random.choice(number_list) for _ in range(batch_size-len(closest_numbers))]\n",
        "\n",
        "    return random_choices"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom EdgeGCNConv Layer\n",
        "An EdgeGCN is a modified (but simpler) version of a GCN that includes the edge embeddings as part of its message passing. For this, we use a simple addition between the node and edge embeddings (after appropriate linear transformations to a matching dimensional space)  in the message passing function. Mathematically, this can be shown as $$m_u^{(l)} = \\mathbf{W_n} h_u + \\mathbf{W_e} e_{uv}$$ where $W_n$ and $W_e$ represent learnable weight matrices. To further increase expressiveness, we also change the aggregation function to addition instead of the default mean. This could help differentiate between graphs with higher number of nodes, potentially corresponding to richer scenes, and graphs with lesser number of nodes.  These changes do not affect the update function itself, which thus stays the same.\n",
        "\n"
      ],
      "metadata": {
        "id": "qRad-aTqvfSq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FTlo2XJPL0YJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.nn import MessagePassing\n",
        "\n",
        "class EdgeGCNConv(MessagePassing):\n",
        "    \"\"\"\n",
        "    A modified version of a GCN layer that also uses edge features.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, edge_dim):\n",
        "        super(EdgeGCNConv, self).__init__(aggr='add')\n",
        "        self.node_mlp = torch.nn.Linear(in_channels, out_channels)\n",
        "        self.edge_mlp = torch.nn.Linear(edge_dim, out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index, edge_attr):\n",
        "        return self.propagate(edge_index, x=x, edge_attr=edge_attr)\n",
        "\n",
        "    def message(self, x_j, edge_attr):\n",
        "\t   # Updated message passing function\n",
        "        return self.node_mlp(x_j) + self.edge_mlp(edge_attr)\n",
        "\n",
        "    def update(self, aggr_out):\n",
        "        return aggr_out"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main classes\n",
        "Primary SimGNN model class that represents the full model architecture, and the SimGNN trainer class that trains and scores the model on the input dataset. Some functions have been customized for our use. Original code source - https://github.com/gospodima/Extended-SimGNN/tree/master."
      ],
      "metadata": {
        "id": "NFf1B5hi2oS3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0AZ6d0NyHfr"
      },
      "outputs": [],
      "source": [
        "from types import new_class\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm, trange\n",
        "from scipy.stats import spearmanr, kendalltau\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GINConv\n",
        "from torch_geometric.data import DataLoader, Batch\n",
        "from torch_geometric.utils import to_dense_batch, to_dense_adj, degree\n",
        "from torch_geometric.datasets import GEDDataset\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "class SimGNN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    SimGNN: A Neural Network Approach to Fast Graph Similarity Computation\n",
        "    https://arxiv.org/abs/1808.05689\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args, number_of_labels):\n",
        "        \"\"\"\n",
        "        :param args: Arguments object.\n",
        "        :param number_of_labels: Number of node labels.\n",
        "        \"\"\"\n",
        "        super(SimGNN, self).__init__()\n",
        "        self.args = args\n",
        "        self.number_labels = number_of_labels\n",
        "        self.setup_layers()\n",
        "\n",
        "    def calculate_bottleneck_features(self):\n",
        "        \"\"\"\n",
        "        Deciding the shape of the bottleneck layer.\n",
        "        \"\"\"\n",
        "        if self.args.histogram:\n",
        "            self.feature_count = self.args.tensor_neurons + self.args.bins\n",
        "        else:\n",
        "            self.feature_count = self.args.tensor_neurons\n",
        "\n",
        "    def setup_layers(self):\n",
        "        \"\"\"\n",
        "        Creating the layers.\n",
        "        \"\"\"\n",
        "        self.calculate_bottleneck_features()\n",
        "        if self.args.gnn_operator == \"gcn\":\n",
        "            self.convolution_1 = GCNConv(self.number_labels, self.args.filters_1)\n",
        "            self.convolution_2 = GCNConv(self.args.filters_1, self.args.filters_2)\n",
        "            self.convolution_3 = GCNConv(self.args.filters_2, self.args.filters_3)\n",
        "        elif self.args.gnn_operator == \"egcn\":\n",
        "            self.convolution_1 = EdgeGCNConv(self.number_labels, self.args.filters_1, self.args.edge_dim)\n",
        "            self.convolution_2 = EdgeGCNConv(self.args.filters_1, self.args.filters_2, self.args.edge_dim)\n",
        "            self.convolution_3 = EdgeGCNConv(self.args.filters_2, self.args.filters_3, self.args.edge_dim)\n",
        "        elif self.args.gnn_operator == \"gin\":\n",
        "            nn1 = torch.nn.Sequential(\n",
        "                torch.nn.Linear(self.number_labels, self.args.filters_1),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(self.args.filters_1, self.args.filters_1),\n",
        "                torch.nn.BatchNorm1d(self.args.filters_1),\n",
        "            )\n",
        "\n",
        "            nn2 = torch.nn.Sequential(\n",
        "                torch.nn.Linear(self.args.filters_1, self.args.filters_2),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(self.args.filters_2, self.args.filters_2),\n",
        "                torch.nn.BatchNorm1d(self.args.filters_2),\n",
        "            )\n",
        "\n",
        "            nn3 = torch.nn.Sequential(\n",
        "                torch.nn.Linear(self.args.filters_2, self.args.filters_3),\n",
        "                torch.nn.ReLU(),\n",
        "                torch.nn.Linear(self.args.filters_3, self.args.filters_3),\n",
        "                torch.nn.BatchNorm1d(self.args.filters_3),\n",
        "            )\n",
        "\n",
        "            self.convolution_1 = GINConv(nn1, train_eps=True)\n",
        "            self.convolution_2 = GINConv(nn2, train_eps=True)\n",
        "            self.convolution_3 = GINConv(nn3, train_eps=True)\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unknown GNN-Operator.\")\n",
        "\n",
        "        if self.args.diffpool:\n",
        "            self.attention = DiffPool(self.args)\n",
        "        else:\n",
        "            self.attention = AttentionModule(self.args)\n",
        "\n",
        "        self.tensor_network = TensorNetworkModule(self.args)\n",
        "        self.fully_connected_first = torch.nn.Linear(\n",
        "            self.feature_count, self.args.bottle_neck_neurons\n",
        "        )\n",
        "        self.scoring_layer = torch.nn.Linear(self.args.bottle_neck_neurons, 1)\n",
        "\n",
        "    def calculate_histogram(\n",
        "        self, abstract_features_1, abstract_features_2, batch_1, batch_2\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Calculate histogram from similarity matrix.\n",
        "        :param abstract_features_1: Feature matrix for target graphs.\n",
        "        :param abstract_features_2: Feature matrix for source graphs.\n",
        "        :param batch_1: Batch vector for source graphs, which assigns each node to a specific example\n",
        "        :param batch_1: Batch vector for target graphs, which assigns each node to a specific example\n",
        "        :return hist: Histsogram of similarity scores.\n",
        "        \"\"\"\n",
        "        abstract_features_1, mask_1 = to_dense_batch(abstract_features_1, batch_1)\n",
        "        abstract_features_2, mask_2 = to_dense_batch(abstract_features_2, batch_2)\n",
        "\n",
        "        B1, N1, _ = abstract_features_1.size()\n",
        "        B2, N2, _ = abstract_features_2.size()\n",
        "\n",
        "        mask_1 = mask_1.view(B1, N1)\n",
        "        mask_2 = mask_2.view(B2, N2)\n",
        "        num_nodes = torch.max(mask_1.sum(dim=1), mask_2.sum(dim=1))\n",
        "\n",
        "        scores = torch.matmul(\n",
        "            abstract_features_1, abstract_features_2.permute([0, 2, 1])\n",
        "        ).detach()\n",
        "\n",
        "        hist_list = []\n",
        "        for i, mat in enumerate(scores):\n",
        "            mat = torch.sigmoid(mat[: num_nodes[i], : num_nodes[i]]).view(-1)\n",
        "            hist = torch.histc(mat, bins=self.args.bins)\n",
        "            hist = hist / torch.sum(hist)\n",
        "            hist = hist.view(1, -1)\n",
        "            hist_list.append(hist)\n",
        "\n",
        "        return torch.stack(hist_list).view(-1, self.args.bins)\n",
        "\n",
        "    def convolutional_pass(self, edge_index, features, edge_attr):\n",
        "        \"\"\"\n",
        "        Making convolutional pass.\n",
        "        :param edge_index: Edge indices.\n",
        "        :param features: Feature matrix.\n",
        "        :return features: Abstract feature matrix.\n",
        "        \"\"\"\n",
        "        if self.args.gnn_operator in [\"gcn\", \"gin\"]:\n",
        "            features = self.convolution_1(features, edge_index)\n",
        "        elif self.args.gnn_operator == \"egcn\":\n",
        "            features = self.convolution_1(features, edge_index, edge_attr)\n",
        "        else:\n",
        "            raise NotImplementedError(\"Unknown GNN-Operator.\")\n",
        "        features = F.relu(features)\n",
        "        features = F.dropout(features, p=self.args.dropout, training=self.training)\n",
        "        ## we only use a single convolutional layer for our use case\n",
        "        # features = self.convolution_2(features, edge_index, edge_attr)\n",
        "        # features = F.relu(features)\n",
        "        # features = F.dropout(features, p=self.args.dropout, training=self.training)\n",
        "        # features = self.convolution_3(features, edge_index, edge_attr)\n",
        "        return features\n",
        "\n",
        "    def diffpool(self, abstract_features, edge_index, batch):\n",
        "        \"\"\"\n",
        "        Making differentiable pooling.\n",
        "        :param abstract_features: Node feature matrix.\n",
        "        :param edge_index: Edge indices\n",
        "        :param batch: Batch vector, which assigns each node to a specific example\n",
        "        :return pooled_features: Graph feature matrix.\n",
        "        \"\"\"\n",
        "        x, mask = to_dense_batch(abstract_features, batch)\n",
        "        adj = to_dense_adj(edge_index, batch)\n",
        "        return self.attention(x, adj, mask)\n",
        "\n",
        "    def forward(self, data):\n",
        "        \"\"\"\n",
        "        Forward pass with graphs.\n",
        "        :param data: Data dictionary.\n",
        "        :return score: Similarity score.\n",
        "        \"\"\"\n",
        "        edge_index_1 = data[\"g1\"].edge_index\n",
        "        edge_index_2 = data[\"g2\"].edge_index\n",
        "        features_1 = data[\"g1\"].x\n",
        "        features_2 = data[\"g2\"].x\n",
        "        if hasattr(data[\"g1\"], \"edge_attr\"):\n",
        "            edge_attr_1 = data[\"g1\"].edge_attr\n",
        "            edge_attr_2 = data[\"g2\"].edge_attr\n",
        "        batch_1 = (\n",
        "            data[\"g1\"].batch\n",
        "            if hasattr(data[\"g1\"], \"batch\")\n",
        "            else torch.tensor((), dtype=torch.long).new_zeros(data[\"g1\"].num_nodes)\n",
        "        )\n",
        "        batch_2 = (\n",
        "            data[\"g2\"].batch\n",
        "            if hasattr(data[\"g2\"], \"batch\")\n",
        "            else torch.tensor((), dtype=torch.long).new_zeros(data[\"g2\"].num_nodes)\n",
        "        )\n",
        "\n",
        "        abstract_features_1 = self.convolutional_pass(edge_index_1, features_1, edge_attr_1)\n",
        "        abstract_features_2 = self.convolutional_pass(edge_index_2, features_2, edge_attr_2)\n",
        "\n",
        "        if self.args.histogram:\n",
        "            hist = self.calculate_histogram(\n",
        "                abstract_features_1, abstract_features_2, batch_1, batch_2\n",
        "            )\n",
        "\n",
        "        if self.args.diffpool:\n",
        "            pooled_features_1 = self.diffpool(\n",
        "                abstract_features_1, edge_index_1, batch_1\n",
        "            )\n",
        "            pooled_features_2 = self.diffpool(\n",
        "                abstract_features_2, edge_index_2, batch_2\n",
        "            )\n",
        "        else:\n",
        "            pooled_features_1 = self.attention(abstract_features_1, batch_1)\n",
        "            pooled_features_2 = self.attention(abstract_features_2, batch_2)\n",
        "\n",
        "        scores = self.tensor_network(pooled_features_1, pooled_features_2)\n",
        "\n",
        "        if self.args.histogram:\n",
        "            scores = torch.cat((scores, hist), dim=1)\n",
        "\n",
        "        scores = F.relu(self.fully_connected_first(scores))\n",
        "        score = torch.sigmoid(self.scoring_layer(scores)).view(-1)\n",
        "        return score\n",
        "\n",
        "\n",
        "class SimGNNTrainer(object):\n",
        "    \"\"\"\n",
        "    SimGNN model trainer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args, data):\n",
        "        \"\"\"\n",
        "        :param args: Arguments object.\n",
        "        :param data: Dataset dictionary.\n",
        "        \"\"\"\n",
        "        self.args = args\n",
        "        self.process_dataset(data)\n",
        "        self.setup_model()\n",
        "\n",
        "    def setup_model(self):\n",
        "        \"\"\"\n",
        "        Creating a SimGNN.\n",
        "        \"\"\"\n",
        "        self.model = SimGNN(self.args, self.number_of_labels)\n",
        "\n",
        "    def save(self, model_path=None):\n",
        "        \"\"\"\n",
        "        Saving model.\n",
        "        :param model_path: Path to save model.\n",
        "        \"\"\"\n",
        "        if model_path:\n",
        "            self.args.save = model_path\n",
        "        torch.save(self.model.state_dict(), self.args.save)\n",
        "        print(f\"Model is saved under {self.args.save}.\")\n",
        "\n",
        "    def load(self, model_path=None):\n",
        "        \"\"\"\n",
        "        Loading model.\n",
        "        :param model_path: Path to load model.\n",
        "        \"\"\"\n",
        "        if model_path:\n",
        "            self.args.load = model_path\n",
        "        self.model.load_state_dict(torch.load(self.args.load))\n",
        "        print(f\"Model is loaded from {self.args.load}.\")\n",
        "\n",
        "    def process_dataset(self, data):\n",
        "        \"\"\"\n",
        "        Extract attrributes from dataset dictionary.\n",
        "        :param data: Dataset dictionary\n",
        "        \"\"\"\n",
        "        self.training_graphs = data['training_graphs']\n",
        "        self.testing_graphs = data['testing_graphs']\n",
        "        self.ndist_matrix = data['ndist_matrix']\n",
        "        self.real_data_size = self.ndist_matrix.size(0)\n",
        "        self.number_of_labels = self.training_graphs[0].x.size(1)\n",
        "\n",
        "    def create_batches(self):\n",
        "        \"\"\"\n",
        "        Creating batches from the training graph list.\n",
        "        :return batches: Zipped loaders as list.\n",
        "        \"\"\"\n",
        "        batch_size = self.args.batch_size\n",
        "        # create a dataloader using training graphs with given batch size and shuffle=True\n",
        "        source_loader = DataLoader(self.training_graphs, batch_size=self.args.batch_size, shuffle=True)\n",
        "        new_batch_pair_list = []\n",
        "        for source_batch in source_loader:\n",
        "          # for each source batch, create a corresponding target batch based on \"biased\" random sampling logic\n",
        "          target_batch = Batch.from_data_list(dataset[random_sample_from_closest_vectorized(source_batch[\"i\"], train_indices)])\n",
        "          new_batch_pair_list.append((source_batch, target_batch))\n",
        "\n",
        "        return new_batch_pair_list\n",
        "\n",
        "    def transform(self, data):\n",
        "        \"\"\"\n",
        "        Getting distance for graph pair and grouping with data into dictionary.\n",
        "        :param data: Graph pair.\n",
        "        :return new_data: Dictionary with data.\n",
        "        \"\"\"\n",
        "        new_data = dict()\n",
        "\n",
        "        new_data[\"g1\"] = data[0]\n",
        "        new_data[\"g2\"] = data[1]\n",
        "\n",
        "        # fetch normalized distance values from ndist_matrix\n",
        "        normalized_dist = self.ndist_matrix[\n",
        "            data[0][\"i\"].reshape(-1).tolist(), data[1][\"i\"]\n",
        "        ].tolist()\n",
        "\n",
        "        new_data[\"target\"] = (\n",
        "            torch.from_numpy(np.array(normalized_dist)).view(-1).float()\n",
        "        )\n",
        "        return new_data\n",
        "\n",
        "    def process_batch(self, data):\n",
        "        \"\"\"\n",
        "        Forward pass with a data.\n",
        "        :param data: Data that is essentially pair of batches, for source and target graphs.\n",
        "        :return loss: Loss on the data.\n",
        "        \"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "        data = self.transform(data)\n",
        "        target = data[\"target\"]\n",
        "        prediction = self.model(data)\n",
        "        loss = F.mse_loss(prediction, target, reduction=\"sum\")\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def fit(self):\n",
        "        \"\"\"\n",
        "        Training a model.\n",
        "        \"\"\"\n",
        "        print(\"\\nModel training.\\n\")\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            self.model.parameters(),\n",
        "            lr=self.args.learning_rate,\n",
        "            weight_decay=self.args.weight_decay,\n",
        "        )\n",
        "        self.model.train()\n",
        "\n",
        "        epochs = trange(self.args.epochs, leave=True, desc=\"Epoch\")\n",
        "        self.loss_list = []\n",
        "        loss_list_test = []\n",
        "        for epoch in epochs:\n",
        "\n",
        "            if self.args.plot:\n",
        "                if epoch % 10 == 0:\n",
        "                    self.model.train(False)\n",
        "                    cnt_test = 20\n",
        "                    cnt_train = 100\n",
        "                    t = tqdm(\n",
        "                        total=cnt_test * cnt_train,\n",
        "                        position=2,\n",
        "                        leave=False,\n",
        "                        desc=\"Validation\",\n",
        "                    )\n",
        "                    scores = torch.empty((cnt_test, cnt_train))\n",
        "\n",
        "                    for i, g in enumerate(self.testing_graphs[:cnt_test].shuffle()):\n",
        "                        source_batch = Batch.from_data_list([g] * cnt_train)\n",
        "                        target_batch = Batch.from_data_list(\n",
        "                            self.training_graphs[:cnt_train].shuffle()\n",
        "                        )\n",
        "                        data = self.transform((source_batch, target_batch))\n",
        "                        target = data[\"target\"]\n",
        "                        prediction = self.model(data)\n",
        "\n",
        "                        scores[i] = F.mse_loss(\n",
        "                            prediction, target, reduction=\"none\"\n",
        "                        ).detach()\n",
        "                        t.update(cnt_train)\n",
        "\n",
        "                    t.close()\n",
        "                    loss_list_test.append(scores.mean().item())\n",
        "                    self.model.train(True)\n",
        "\n",
        "            batches = self.create_batches()\n",
        "            main_index = 0\n",
        "            loss_sum = 0\n",
        "            for index, batch_pair in tqdm(\n",
        "                enumerate(batches), total=len(batches), desc=\"Batches\", leave=False\n",
        "            ):\n",
        "                loss_score = self.process_batch(batch_pair)\n",
        "                main_index = main_index + batch_pair[0].num_graphs\n",
        "                loss_sum = loss_sum + loss_score\n",
        "            loss = loss_sum / main_index\n",
        "            epochs.set_description(\"Epoch (Loss=%g)\" % round(loss, 5))\n",
        "            self.loss_list.append(loss)\n",
        "\n",
        "        if self.args.plot:\n",
        "            plt.plot(self.loss_list, label=\"Train\")\n",
        "            plt.plot(\n",
        "                [*range(0, self.args.epochs, 10)], loss_list_test, label=\"Validation\"\n",
        "            )\n",
        "            plt.ylim([0, 0.01])\n",
        "            plt.legend()\n",
        "            filename = self.args.dataset\n",
        "            filename += \"_\" + self.args.gnn_operator\n",
        "            if self.args.diffpool:\n",
        "                filename += \"_diffpool\"\n",
        "            if self.args.histogram:\n",
        "                filename += \"_hist\"\n",
        "            filename = filename + str(self.args.epochs) + \".pdf\"\n",
        "            plt.savefig(filename)\n",
        "\n",
        "    def measure_time(self):\n",
        "        import time\n",
        "\n",
        "        self.model.eval()\n",
        "        count = len(self.testing_graphs) * len(self.training_graphs)\n",
        "\n",
        "        t = np.empty(count)\n",
        "        i = 0\n",
        "        tq = tqdm(total=count, desc=\"Graph pairs\")\n",
        "        for g1 in self.testing_graphs:\n",
        "            for g2 in self.training_graphs:\n",
        "                source_batch = Batch.from_data_list([g1])\n",
        "                target_batch = Batch.from_data_list([g2])\n",
        "                data = self.transform((source_batch, target_batch))\n",
        "\n",
        "                start = time.process_time()\n",
        "                self.model(data)\n",
        "                t[i] = time.process_time() - start\n",
        "                i += 1\n",
        "                tq.update()\n",
        "        tq.close()\n",
        "\n",
        "        print(\n",
        "            \"Average time (ms): {}; Standard deviation: {}\".format(\n",
        "                round(t.mean() * 1000, 5), round(t.std() * 1000, 5)\n",
        "            )\n",
        "        )\n",
        "\n",
        "    def score(self):\n",
        "        \"\"\"\n",
        "        Scoring.\n",
        "        \"\"\"\n",
        "        print(\"\\n\\nModel evaluation.\\n\")\n",
        "        self.model.eval()\n",
        "\n",
        "        self.scores = np.empty((len(self.testing_graphs), len(self.training_graphs)))\n",
        "        ground_truth = np.empty((len(self.testing_graphs), len(self.training_graphs)))\n",
        "        prediction_mat = np.empty((len(self.testing_graphs), len(self.training_graphs)))\n",
        "\n",
        "        self.rho_list = []\n",
        "        self.tau_list = []\n",
        "        self.prec_at_10_list = []\n",
        "        self.prec_at_20_list = []\n",
        "\n",
        "        t = tqdm(total=len(self.testing_graphs) * len(self.training_graphs))\n",
        "\n",
        "        for i, g in enumerate(self.testing_graphs):\n",
        "          try:\n",
        "            source_batch = Batch.from_data_list([g] * len(self.training_graphs))\n",
        "            target_batch = Batch.from_data_list(self.training_graphs)\n",
        "\n",
        "            data = self.transform((source_batch, target_batch))\n",
        "            target = data[\"target\"]\n",
        "            ground_truth[i] = target\n",
        "            prediction = self.model(data)\n",
        "            prediction_mat[i] = prediction.detach().numpy()\n",
        "\n",
        "            self.scores[i] = (\n",
        "                F.mse_loss(prediction, target, reduction=\"none\").detach().numpy()\n",
        "            )\n",
        "\n",
        "            self.rho_list.append(\n",
        "                calculate_ranking_correlation(\n",
        "                    spearmanr, prediction_mat[i], ground_truth[i]\n",
        "                )\n",
        "            )\n",
        "            self.tau_list.append(\n",
        "                calculate_ranking_correlation(\n",
        "                    kendalltau, prediction_mat[i], ground_truth[i]\n",
        "                )\n",
        "            )\n",
        "            self.prec_at_10_list.append(\n",
        "                calculate_prec_at_k(10, prediction_mat[i], ground_truth[i])\n",
        "            )\n",
        "            self.prec_at_20_list.append(\n",
        "                calculate_prec_at_k(20, prediction_mat[i], ground_truth[i])\n",
        "            )\n",
        "\n",
        "            t.update(len(self.training_graphs))\n",
        "          except KeyboardInterrupt as e:\n",
        "            raise e\n",
        "          except:\n",
        "            print(\"Ignoring error\")\n",
        "            continue\n",
        "\n",
        "        self.rho = np.mean(self.rho_list).item()\n",
        "        self.tau = np.mean(self.tau_list).item()\n",
        "        self.prec_at_10 = np.mean(self.prec_at_10_list).item()\n",
        "        self.prec_at_20 = np.mean(self.prec_at_20_list).item()\n",
        "        self.model_error = np.mean(self.scores).item()\n",
        "        self.print_evaluation()\n",
        "\n",
        "    def print_evaluation(self):\n",
        "        \"\"\"\n",
        "        Printing the error rates.\n",
        "        \"\"\"\n",
        "        print(\"\\nmse(10^-3): \" + str(round(self.model_error * 1000, 5)) + \".\")\n",
        "        print(\"Spearman's rho: \" + str(round(self.rho, 5)) + \".\")\n",
        "        print(\"Kendall's tau: \" + str(round(self.tau, 5)) + \".\")\n",
        "        print(\"p@10: \" + str(round(self.prec_at_10, 5)) + \".\")\n",
        "        print(\"p@20: \" + str(round(self.prec_at_20, 5)) + \".\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5E4AxrC20bH"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hHqm_vhy8bv"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "\n",
        "def get_default_parameters():\n",
        "    \"\"\"\n",
        "    Returns an argparse.Namespace with the default parameters.\n",
        "    :return args: Default parameters.\n",
        "    \"\"\"\n",
        "    default_parameters = argparse.Namespace(\n",
        "        gnn_operator=\"egcn\",\n",
        "        epochs=100,\n",
        "        filters_1=64,\n",
        "        filters_2=64,\n",
        "        filters_3=64,\n",
        "        tensor_neurons=64,\n",
        "        bottle_neck_neurons=64,\n",
        "        batch_size=32,\n",
        "        bins=64,\n",
        "        dropout=0.0,\n",
        "        learning_rate=0.001,\n",
        "        weight_decay=5e-4,\n",
        "        histogram=True,\n",
        "        diffpool=False,\n",
        "        plot=False,\n",
        "        synth=False,\n",
        "        save=None,\n",
        "        load=None,\n",
        "        measure_time=False,\n",
        "        notify=False,\n",
        "        edge_dim=768,\n",
        "    )\n",
        "    return default_parameters\n",
        "\n",
        "args = get_default_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZCWVAWG34c-"
      },
      "source": [
        "# Create Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgfJHZdS0nTS"
      },
      "outputs": [],
      "source": [
        "output_folder_path = f\"/content/drive/Shareddrives/CS224W/outputs/991_double_fixed/\"\n",
        "df = pd.read_csv(os.path.join(output_folder_path, \"df_final.csv\")).drop(columns=[\"Unnamed: 0\"])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tCk89RJZw-j"
      },
      "outputs": [],
      "source": [
        "dataset = CustomDataset(df, output_folder_path)\n",
        "len(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xco5NZdcPwl2"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# train test split\n",
        "random.seed(42)\n",
        "train_indices = random.sample(range(len(dataset)), int(0.95 * len(dataset)))\n",
        "test_indices = [i for i in range(len(dataset)) if i not in train_indices]\n",
        "training_graphs = dataset[train_indices]\n",
        "testing_graphs = dataset[test_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfBuUmV6Q1lA"
      },
      "outputs": [],
      "source": [
        "data = {\n",
        "    'training_graphs': training_graphs,\n",
        "    'testing_graphs': testing_graphs,\n",
        "    'ndist_matrix': dataset.ndist_matrix,\n",
        "    'output_folder_path': output_folder_path\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-FNsh894DVa"
      },
      "source": [
        "# Train and Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDTwObx10bjJ"
      },
      "outputs": [],
      "source": [
        "trainer = SimGNNTrainer(args, data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R9LHA9aP4dOc"
      },
      "outputs": [],
      "source": [
        "trainer.fit()\n",
        "plt.plot(trainer.loss_list)\n",
        "trainer.save(output_folder_path+'saved_model_egcn_95_split_1_layer_biased.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF1fCx8q4exM"
      },
      "source": [
        "# Load and Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GmQBzJJ8KI-6"
      },
      "outputs": [],
      "source": [
        "trainer.load(output_folder_path+'saved_model_egcn_95_split_1_layer_biased.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dvgNVRi0TfCj"
      },
      "outputs": [],
      "source": [
        "trainer.score()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test on query graph"
      ],
      "metadata": {
        "id": "Hq_V_A-P4rz-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoYyY6Q_bdVo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from matplotlib.pyplot import imread\n",
        "\n",
        "def display_graph(graph_data, image_path, coefs=None):\n",
        "    # Convert JSON to NetworkX graph\n",
        "    G = nx.node_link_graph(graph_data, edges=\"edges\", directed=True)\n",
        "\n",
        "    # Set up layout for the graph\n",
        "    pos = nx.shell_layout(G)\n",
        "\n",
        "    # Create a figure with two subplots\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    # Left subplot: Display the image\n",
        "    image = imread(image_path)\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].axis(\"off\")  # Hide axes for the image\n",
        "    axes[0].set_title(image_path.split(\"/\")[-1])\n",
        "\n",
        "    # Right subplot: Display the graph\n",
        "    if coefs is not None:\n",
        "        vmin = coefs.min().item() - 0.005\n",
        "        vmax = coefs.max().item() + 0.005\n",
        "        colors = coefs.tolist()\n",
        "        nodes = nx.draw_networkx_nodes(\n",
        "            G,\n",
        "            pos,\n",
        "            node_size=300,\n",
        "            node_color=colors,\n",
        "            cmap=plt.cm.Reds,\n",
        "            vmin=vmin,\n",
        "            vmax=vmax,\n",
        "            edgecolors=\"black\",\n",
        "            ax=axes[1],\n",
        "        )\n",
        "        cbar = plt.colorbar(nodes, ax=axes[1])\n",
        "        cbar.set_label('Coefficient Values')\n",
        "        cbar.ax.tick_params(labelsize=8)  # Optional: Set the tick label size\n",
        "\n",
        "    else:\n",
        "        nx.draw_networkx_nodes(G, pos, node_size=300, node_color=\"skyblue\", edgecolors=\"black\", ax=axes[1])\n",
        "    nx.draw_networkx_edges(G, pos, arrowstyle=\"->\", arrowsize=30, ax=axes[1])\n",
        "    nx.draw_networkx_labels(G, pos, verticalalignment=\"center\", horizontalalignment=\"center\", font_color=\"blue\",\n",
        "                            labels={node['id']: f\"\\n\\n\\n\\n\\n\\n{node['feature']}\" for node in graph_data[\"nodes\"]}, font_size=8, ax=axes[1])\n",
        "\n",
        "    edge_labels = {(link[\"source\"], link[\"target\"]): link[\"type\"] for link in graph_data[\"edges\"]}\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=axes[1])\n",
        "    axes[1].set_title(\"Scene Graph Visualization\")\n",
        "    axes[1].axis(\"off\")  # Hide axes for the graph\n",
        "\n",
        "    # Adjust spacing and display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_nearest_neighbors(g):\n",
        "\n",
        "    image_dataset_folder = \"/content/drive/Shareddrives/CS224W/Dataset/all\"\n",
        "\n",
        "    # create pairs with all graphs in the dataset\n",
        "    source_batch = Batch.from_data_list([g] * len(dataset))\n",
        "    target_batch = Batch.from_data_list(dataset)\n",
        "\n",
        "    # get model predictions\n",
        "    data = trainer.transform((source_batch, target_batch))\n",
        "    target = data[\"target\"]\n",
        "    ground_truth = target\n",
        "    prediction = trainer.model(data)\n",
        "    prediction_mat = prediction.detach().numpy()\n",
        "\n",
        "    # get attention coefficients\n",
        "    features = trainer.model.convolutional_pass(g.edge_index, g.x, g.edge_attr)\n",
        "    coefs = trainer.model.attention.get_coefs(features)\n",
        "\n",
        "    # load graph from json file\n",
        "    query_image_path = os.path.join(image_dataset_folder, g.image_filename)\n",
        "    with open(os.path.join(output_folder_path, g.graph_filename), 'r') as f:\n",
        "        query_graph_data = json.load(f)\n",
        "\n",
        "    display_graph(query_graph_data, query_image_path, coefs)\n",
        "\n",
        "    # Retrieve and display top-k images and graphs\n",
        "    print(f\"\\nTop {5} Retrieved Images and Graphs:\")\n",
        "    retrieved_indices = np.argsort(prediction_mat)[:5]\n",
        "    for i, idx in enumerate(retrieved_indices):\n",
        "        g_ = dataset[idx]\n",
        "        features = trainer.model.convolutional_pass(g_.edge_index, g_.x, g_.edge_attr)\n",
        "        coefs = trainer.model.attention.get_coefs(features)\n",
        "        retrieved_image_path = os.path.join(image_dataset_folder, g_.image_filename)\n",
        "        with open(os.path.join(output_folder_path, g_.graph_filename), 'r') as f:\n",
        "            retrieved_graph_data = json.load(f)\n",
        "\n",
        "        print(f\"\\nRetrieved Image #{i + 1}:\")\n",
        "        print(f\"Ground Truth: {target[idx]}\")\n",
        "        print(f\"Prediction: {prediction_mat[idx]}\")\n",
        "        display_graph(retrieved_graph_data, retrieved_image_path, coefs)"
      ],
      "metadata": {
        "id": "DpykSD8_4827"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qynw_r3WLaRT"
      },
      "outputs": [],
      "source": [
        "index = 500\n",
        "g = dataset[index]\n",
        "# g = testing_graphs[0]\n",
        "\n",
        "retrieve_nearest_neighbors(g)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "Y08trPfN2lMl",
        "z7EHWHZJ2ouo",
        "5zZdoNrg2sEt",
        "nmSwIUU0u8NY",
        "qRad-aTqvfSq",
        "NFf1B5hi2oS3",
        "h5E4AxrC20bH",
        "ZZCWVAWG34c-",
        "7-FNsh894DVa",
        "lF1fCx8q4exM",
        "Hq_V_A-P4rz-"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}