{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "_4-Ij_wEW_Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade google-cloud-aiplatform\n",
        "!gcloud auth application-default login"
      ],
      "metadata": {
        "id": "K1_dSCGhw4S8",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth application-default set-quota-project graph-localization"
      ],
      "metadata": {
        "id": "jpWiD-uzOiR-",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "XK_4m9_qJkLf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import random\n",
        "\n",
        "import base64\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel, Part, SafetySetting, Image\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from pprint import pprint\n",
        "import json\n",
        "from json.decoder import JSONDecodeError\n",
        "import os\n",
        "import numpy as np\n",
        "from time import sleep"
      ],
      "metadata": {
        "id": "4lhMsrjbXEpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for generating scene graphs using Gemini\n"
      ],
      "metadata": {
        "id": "9O_0P1K8D9pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "Task: Static Environmental Features Analysis\n",
        "\n",
        "Analyze the provided image to identify the following types of objects:\n",
        "- buildings\n",
        "- parks\n",
        "- trees\n",
        "- overpasses\n",
        "- bridges\n",
        "- the road in the line of sight of the camera\n",
        "\n",
        "Ignore any temporary objects, such as vehicles or people.\n",
        "\n",
        "For each of these objects in the scene, generate a structured JSON output as described below. Each object must have:\n",
        "\n",
        "A unique node ID (e.g., \"brick_building_2_story\", \"oak_tree_left\", \"road_center\", etc.).\n",
        "A description that excludes spatial information but focuses on the object's attributes.\n",
        "Defined relationships with every other object using the following spatial relations:\n",
        "is_left_of\n",
        "is_right_of\n",
        "is_above\n",
        "is_below\n",
        "is_in_front_of\n",
        "is_behind\n",
        "\n",
        "If the type of the object is a building, write the feature as main building materal and estimated number of floors, such as\n",
        "\"glass, 12 stories\"\n",
        "\n",
        "Example Output Format:\n",
        "{\n",
        "  \"node_id_1\": {\n",
        "    \"type\": \"the type of object, e.g. building or overpass\",\n",
        "    \"feature\": \"as described above. Leave as empty string if not a building.\",\n",
        "    \"is_left_of\": [\"node_id_2\", \"node_id_3\"],\n",
        "    \"is_right_of\": [\"node_id_4\"],\n",
        "    \"is_above\": [\"node_id_5\"],\n",
        "    \"is_below\": [],\n",
        "    \"is_in_front_of\": [\"node_id_3\"],\n",
        "    \"is_behind\": [\"node_id_4\"]\n",
        "  },\n",
        "  \"node_id_2\": {\n",
        "    ...\n",
        "  },\n",
        "  ...\n",
        "}\n",
        "Requirements:\n",
        "Ensure that every object has defined relationships with all other objects using the specified spatial relation types.\n",
        "Maintain consistency and completeness for all relationships.\n",
        "\"\"\"\n",
        "\n",
        "generation_config = {\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"temperature\": 0.1,\n",
        "    \"top_p\": 0.95,\n",
        "    \"response_mime_type\": \"application/json\",\n",
        "}\n",
        "\n",
        "safety_settings = [\n",
        "    SafetySetting(\n",
        "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
        "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=SafetySetting.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
        "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=SafetySetting.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
        "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "    ),\n",
        "    SafetySetting(\n",
        "        category=SafetySetting.HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
        "        threshold=SafetySetting.HarmBlockThreshold.OFF\n",
        "    ),\n",
        "]\n",
        "\n",
        "def generate(prompt_content) -> str:\n",
        "    \"\"\"\n",
        "    Prompt the AI and return the answer as a string.\n",
        "    \"\"\"\n",
        "    vertexai.init(project=\"graph-localization\", location=\"us-central1\")\n",
        "    model = GenerativeModel(\"gemini-1.5-pro-002\")\n",
        "    responses = model.generate_content(\n",
        "        prompt_content,\n",
        "        generation_config=generation_config,\n",
        "        safety_settings=safety_settings,\n",
        "        stream=True,\n",
        "    )\n",
        "\n",
        "    output = \"\"\n",
        "    for response in responses:\n",
        "        output += response.text\n",
        "    return output\n",
        "\n",
        "\n",
        "def convert_format(input_data, remove_doubles=True) -> dict:\n",
        "    \"\"\"\n",
        "    Convert the decoded JSON object to an object readable by NetworkX\n",
        "    \"\"\"\n",
        "\n",
        "    nodes = []\n",
        "    edges = set()\n",
        "\n",
        "    # When adding an edge, add the node pair (excluding the edge type)\n",
        "    edges_without_type = set()\n",
        "\n",
        "    # Mapping for edge type transformation\n",
        "    # We do this as it is simpler to only have 3 relations than 6\n",
        "    opposite_relation = {\n",
        "        \"is_right_of\": \"is_left_of\",\n",
        "        \"is_behind\": \"is_in_front_of\",\n",
        "        \"is_below\": \"is_above\",\n",
        "        \"is_above\": \"is_below\",\n",
        "        \"is_in_front_of\": \"is_behind\",\n",
        "        \"is_left_of\": \"is_right_of\",\n",
        "    }\n",
        "\n",
        "    # Extract nodes\n",
        "    for source, attributes in input_data.items():\n",
        "        # Add the node with its feature to the nodes list\n",
        "        nodes.append({\"id\": source, \"feature\": attributes[\"type\"] + \", \" + attributes[\"feature\"]})\n",
        "\n",
        "        # Extract spatial relationships and convert them into edges\n",
        "        for relation, targets in attributes.items():\n",
        "            # Skip the feature and type attributes as they are not relations.\n",
        "            if relation == \"feature\" or relation == \"type\":\n",
        "                continue\n",
        "            # Add edges for each spatial relationship\n",
        "            for target in targets:\n",
        "                if (source, target) in edges_without_type or (target, source) in edges_without_type:\n",
        "                    # Skip adding the same edge twice\n",
        "                    continue\n",
        "                edges_without_type.add((source, target))\n",
        "                edges.add((source, relation, target))\n",
        "                edges.add((target, opposite_relation[relation], source))\n",
        "\n",
        "    # Convert the set of edges to a list of dictionaries\n",
        "    edges = [{\"source\": source, \"type\": edge_type, \"target\": target} for source, edge_type, target in edges]\n",
        "\n",
        "    output = {\n",
        "        \"nodes\": nodes,\n",
        "        \"edges\": edges\n",
        "    }\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "v5yLIXWXxVBv",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for adding feature embeddings to nodes and edges"
      ],
      "metadata": {
        "id": "yXb7FadX9lCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def embed_text(texts) -> list[list[float]]:\n",
        "    \"\"\"\n",
        "    Embeds texts with a pre-trained, foundational model using the API.\n",
        "\n",
        "    Returns:\n",
        "        A list of embedding vectors for each input text\n",
        "    \"\"\"\n",
        "\n",
        "    task = \"SEMANTIC_SIMILARITY\"  # This task matches our goal the best\n",
        "    model = TextEmbeddingModel.from_pretrained(\"text-embedding-005\")\n",
        "\n",
        "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
        "    embeddings = model.get_embeddings(inputs)\n",
        "\n",
        "    return [embedding.values for embedding in embeddings]\n",
        "\n",
        "def add_embeddings(graph_data) -> None:\n",
        "    \"\"\"\n",
        "    Add embeddings to nodes and edges in the graph data.\n",
        "    \"\"\"\n",
        "\n",
        "    # Add the embedding attribute to each node using only one API request\n",
        "    node_features = [node[\"feature\"] for node in graph_data[\"nodes\"]]\n",
        "    node_embeddings = embed_text(node_features)\n",
        "    for node, embedding in zip(graph_data[\"nodes\"], node_embeddings):\n",
        "        node[\"embedding\"] = embedding\n",
        "\n",
        "    # Add embeddings to edges. Since edge features have less variety, we make\n",
        "    # sure to only get the embedding once for each edge type.\n",
        "    if graph_data[\"edges\"]:\n",
        "      edge_types = set([edge[\"type\"] for edge in graph_data[\"edges\"]])\n",
        "      edge_embeddings = embed_text(edge_types)\n",
        "      embedding_map = {edge_type: embedding for edge_type, embedding in zip(edge_types, edge_embeddings)}\n",
        "      for edge in graph_data[\"edges\"]:\n",
        "          edge[\"embedding\"] = embedding_map[edge[\"type\"]]\n"
      ],
      "metadata": {
        "id": "tFI3ghiS9qdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code for visualizing a graph using NetworkX"
      ],
      "metadata": {
        "id": "JiTq8Ppxxblp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_graph(graph_data, image_path) -> None:\n",
        "    \"\"\"\n",
        "    Dsiplay a figure containing the image and its graph.\n",
        "    \"\"\"\n",
        "\n",
        "    G = nx.node_link_graph(graph_data, edges=\"edges\", directed=True)\n",
        "\n",
        "    # Set up layout for the graph\n",
        "    pos = nx.shell_layout(G)\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "\n",
        "    # Left subplot: Display the image\n",
        "    image = imread(image_path)\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].axis(\"off\")  # Hide axes for the image\n",
        "    axes[0].set_title(image_path.split(\"/\")[-1])\n",
        "\n",
        "    # Right subplot: Display the graph\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=300, node_color=\"skyblue\", edgecolors=\"black\", ax=axes[1])\n",
        "    nx.draw_networkx_labels(G, pos, verticalalignment=\"center\", horizontalalignment=\"center\",\n",
        "                            labels={node['id']: f\"\\n\\n\\n\\n\\n\\n{node['feature']}\" for node in graph_data[\"nodes\"]}, font_size=8, ax=axes[1])\n",
        "    nx.draw_networkx_edges(G, pos, arrowstyle=\"->\", arrowsize=30, ax=axes[1])\n",
        "    edge_labels = {(link[\"source\"], link[\"target\"]): link[\"type\"] for link in graph_data[\"edges\"]}\n",
        "    nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_size=8, label_pos=0.3, ax=axes[1])\n",
        "    axes[1].axis(\"off\")  # Hide axes for the graph\n",
        "\n",
        "    # Adjust spacing and display the plot\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "i9oYsKKHO1q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate a dataset of graphs\n",
        "\n"
      ],
      "metadata": {
        "id": "rV0emdEu56--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_graph(image_path, embeddings=True, plot=False, remove_multiple_edges=True, attempt=1) -> dict | None:\n",
        "    \"\"\"\n",
        "    Generate a graph for the given image and return the graph data.\n",
        "\n",
        "    Args:\n",
        "        image_path (str): The path to the image file.\n",
        "        embeddings (bool, optional): Whether to add embeddings to the nodes and edges.\n",
        "        plot (bool, optional): Whether to display the graph.\n",
        "        remove_multiple_edges (bool, optional): Whether to only use a maximum of one edges between nodes.\n",
        "            This can be useful if the model insists on having multiple edges between nodes.\n",
        "        attempt (int, optional): The attempt number, used for retrying in case of failure.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the graph data if successful, otherwise None.\n",
        "    \"\"\"\n",
        "    if attempt > 5:\n",
        "      return None\n",
        "\n",
        "    # Prompt the LLM\n",
        "    image = Image.load_from_file(image_path)\n",
        "    try:\n",
        "      response = generate([image, prompt])\n",
        "    except Exception as e:\n",
        "      # This can sometimes happen due to API limits - waiting usually helps.\n",
        "      print(\"Failed to get a response from the LLM:\")\n",
        "      print(e)\n",
        "      print(\"Waiting 5 seconds and Trying again!\")\n",
        "      sleep(5)\n",
        "      return generate_graph(image_path, embeddings, plot, remove_multiple_edges, attempt+1)\n",
        "\n",
        "    # Convert into a NetworkX readable format\n",
        "    try:\n",
        "      graph_data = convert_format(json.loads(response), remove_doubles=remove_multiple_edges)\n",
        "    except JSONDecodeError:\n",
        "      print(\"Got invalid JSON. Trying again!\")\n",
        "      return generate_graph(image_path, embeddings, plot, remove_multiple_edges, attempt+1)\n",
        "    except Exception as e:\n",
        "      print(\"Got an error when generating graph:\")\n",
        "      print(e)\n",
        "      print(\"Ignoring and trying again!\")\n",
        "      return generate_graph(image_path, embeddings, plot, remove_multiple_edges, attempt+1)\n",
        "\n",
        "    # Add embeddings if requested\n",
        "    if embeddings:\n",
        "      try:\n",
        "        add_embeddings(graph_data)\n",
        "      except Exception as e:\n",
        "        print(\"Got an error when adding embeddings:\")\n",
        "        print(e)\n",
        "        print(\"Ignoring and trying again!\")\n",
        "        return generate_graph(image_path, embeddings, plot, remove_multiple_edges, attempt+1)\n",
        "\n",
        "    if plot:\n",
        "      display_graph(graph_data, image_path)\n",
        "\n",
        "    return graph_data"
      ],
      "metadata": {
        "id": "dAX0aPM77i-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.io import loadmat\n",
        "\n",
        "# Load the .mat files containing coordinates in both lat_long and cartesian coordinates.\n",
        "# The cartesian coordinates are constructed so that the euclidean distance between two points is the real-world distance in meters.\n",
        "lat_long_data = loadmat('/content/drive/Shareddrives/CS224W/Dataset/GPS_Long_Lat_Compass.mat')\n",
        "cartesian_data = loadmat('/content/drive/Shareddrives/CS224W/Dataset/Cartesian_Location_Coordinates.mat')\n",
        "\n",
        "# Get all available images\n",
        "img_dir = \"/content/drive/Shareddrives/CS224W/Dataset/all\"\n",
        "number_of_files = len(os.listdir(img_dir))\n",
        "sorted_img_list = sorted(os.listdir(img_dir))\n",
        "print(\"Number of images: \", len(sorted_img_list))"
      ],
      "metadata": {
        "id": "xq_LcmAVEC0Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Create maching lists of coordinates, both lat-long and cartesian.\n",
        "lat_longs = []\n",
        "cartesians = []\n",
        "for filename in sorted_img_list:\n",
        "    index = int(filename.split(\"_\")[0]) - 1\n",
        "    lat_longs.append(lat_long_data[\"GPS_Compass\"][index])\n",
        "    cartesians.append(cartesian_data[\"XYZ_Cartesian\"][index])\n",
        "\n",
        "# Merge these lists into a dataframe.\n",
        "full_df = pd.DataFrame(\n",
        "    np.concatenate((lat_longs, cartesians), axis=1),\n",
        "    columns=['GPS_Lat', 'GPS_Long', 'Compass', 'X', 'Y', 'Z'],\n",
        "    )\n",
        "full_df['image_filename'] = sorted_img_list\n",
        "\n",
        "# Adding image direction simplifies any filtering we might want to do later.\n",
        "full_df['image_direction'] = [int(filename.split(\".\")[0][-1]) for filename in full_df['image_filename']]\n",
        "\n",
        "# Graphs share the same filename, just a different file type.\n",
        "full_df['graph_filename'] = [filename.replace(\".jpg\", \".json\") for filename in full_df['image_filename']]\n",
        "\n",
        "# Create output folder\n",
        "date_time_str = datetime.now().strftime(\"%Y-%m-%d_%H:%M:%S\")\n",
        "output_folder_path = f\"/content/drive/Shareddrives/CS224W/outputs/{date_time_str}\"\n",
        "os.makedirs(output_folder_path)\n",
        "\n",
        "# Filter for images facing in the driving direciton\n",
        "df = full_df.loc[full_df[\"image_direction\"] == 4]\n",
        "print(\"Number of images in the chosen direction: \", len(df))\n",
        "\n",
        "# Only keep images from Pittsburgh\n",
        "df = df[(df['GPS_Long']<=-79.8)&(df['GPS_Long']>=-80.01)&(df['GPS_Lat']>=40)]\n",
        "print(\"Number of images from Pittsburgh: \", len(df))\n",
        "\n",
        "# Only keep a certain number of images\n",
        "NUMBER_OF_ROWS = 1000\n",
        "df = df.head(NUMBER_OF_ROWS)\n",
        "\n",
        "# Save a draft of the dataframe now. Some graphs might prove problematic to\n",
        "# generate, and might get skipped. We collect the problematic image indices\n",
        "# and delete those rows before saving the final dataframe.\n",
        "df.to_csv(os.path.join(output_folder_path, \"df_initial.csv\"))\n",
        "failed_row_indices = []\n",
        "\n",
        "# In case of previous failure, set path manually here and set continue_from to\n",
        "# where to resume from.\n",
        "# output_folder_path = \"/content/drive/Shareddrives/CS224W/outputs/2024-12-08_02:36:13\"\n",
        "continue_from = 0\n",
        "\n",
        "# Generate a graph for each image\n",
        "i = 0  # iterrows returns row index which is not suitable for progress updates\n",
        "for row_index, row in df.iterrows():\n",
        "  if (i:=i+1) < continue_from:\n",
        "    continue\n",
        "\n",
        "  print(f\"Generating graph for {row['image_filename']} ({(i)}/{len(df)})\")\n",
        "\n",
        "  # Generate Graph data\n",
        "  image_path = os.path.join(img_dir, row['image_filename'])\n",
        "  graph_data = generate_graph(image_path, embeddings=True, plot=False)\n",
        "\n",
        "  if graph_data is None:\n",
        "    failed_row_indices.append(row_index)\n",
        "    print(f\"Failed to generate graph for {row['image_filename']}\")\n",
        "    continue\n",
        "\n",
        "  # Save Graph data\n",
        "  graph_path = os.path.join(output_folder_path, row['graph_filename'])\n",
        "  json_object = json.dumps(graph_data, indent=4)\n",
        "  with open(graph_path, \"w\") as outfile:\n",
        "    outfile.write(json_object)\n",
        "\n",
        "  print()\n",
        "\n",
        "# Delete all failed rows and save the final dataframe\n",
        "print(\"Failed rows: \", failed_row_indices)\n",
        "df = df.drop(failed_row_indices)\n",
        "df.to_csv(os.path.join(output_folder_path, \"df_final.csv\"))"
      ],
      "metadata": {
        "id": "0dNqbYgCyF9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify that all output files are present\n",
        "files = set(os.listdir(output_folder_path))\n",
        "for filename in df['graph_filename']:\n",
        "  if filename not in files:\n",
        "    print(f\"Missing {filename}\")"
      ],
      "metadata": {
        "id": "s1SK4Cp8Y8Ls"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}